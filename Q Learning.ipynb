{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7d7589-ae4e-4def-9ba8-e465a0a1194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93928e-04f3-4eff-9e52-4ccd478009f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\"Q-Learning algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes an agent that learns a policy via Q-Learning algorithm [1]\n",
    "        to solve the task at hand (LunarLanderv2).\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-3  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns an action, conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: Action to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_logits = self.net(state)\n",
    "        \n",
    "        potentials = F.softmax(action_logits, dim=1)\n",
    "        # print(potentials)\n",
    "\n",
    "        action = torch.multinomial(potentials, 1)[0].item()\n",
    "        # print(action)\n",
    "\n",
    "        self.probs.append(torch.log(potentials[0][action]))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        # minimize -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob * delta * (-1)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
